{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b96c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Define the directory path relative to the current working directory\n",
    "dirname = os.path.join(cwd, 'Project 1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea06a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoding of  City\n",
      "Target encoding of  State\n",
      "Target encoding of  Bank\n",
      "Target encoding of  BankState\n",
      "Target encoding of  RevLineCr\n",
      "One-hot encoding of  LowDoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\ml-spring-2023\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard scale of  Zip\n",
      "Standard scale of  NAICS\n",
      "Standard scale of  Term\n",
      "Standard scale of  NoEmp\n",
      "Standard scale of  NewExist\n",
      "Standard scale of  CreateJob\n",
      "Standard scale of  RetainedJob\n",
      "Standard scale of  FranchiseCode\n",
      "Standard scale of  UrbanRural\n",
      "Standard scale of  DisbursementGross\n",
      "Standard scale of  BalanceGross\n",
      "Standard scale of  GrAppv\n",
      "Standard scale of  SBA_Appv\n",
      "Standard scale of  Is_urban\n",
      "Standard scale of  Retained_Created_Job_Ratio\n",
      "Standard scale of  Is_low_doc\n",
      "Standard scale of  Is_rev_line\n",
      "Standard scale of  Loan_Gross_Ratio\n",
      "Standard scale of  SBA_Loan_Gross_Ratio\n",
      "Standard scale of  Is_franchise\n",
      "Standard scale of  LogDisbursementGross\n",
      "Standard scale of  LogSBAApprovalAmount\n",
      "Standard scale of  BankOriginatedLoan\n",
      "Standard scale of  LoanToIncomeRatio\n",
      "Standard scale of  LoanToOwnerRatio\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\users\\user\\ml-spring-2023\\lib\\site-packages\\pandas\\core\\indexes\\base.py:2898\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas\\_libs\\index.pyx:70\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\index.pyx:101\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:1675\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:1683\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'index'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 145\u001b[0m\n\u001b[0;32m    143\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(datafilepath)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# calling the function\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m answer\u001b[38;5;241m=\u001b[39m\u001b[43mproject_1_scoring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# printing the answer\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "Cell \u001b[1;32mIn[6], line 133\u001b[0m, in \u001b[0;36mproject_1_scoring\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    131\u001b[0m y_pred_prob \u001b[38;5;241m=\u001b[39m logreg\u001b[38;5;241m.\u001b[39mpredict_proba(X_trn)\n\u001b[0;32m    132\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (y_pred_prob[:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint16)\n\u001b[1;32m--> 133\u001b[0m answer_dataframe \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[43mX_trn\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m    134\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m:y_pred,\n\u001b[0;32m    135\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability_0\u001b[39m\u001b[38;5;124m\"\u001b[39m:y_pred_prob[:,\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    136\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability_1\u001b[39m\u001b[38;5;124m\"\u001b[39m:y_pred_prob[:,\u001b[38;5;241m1\u001b[39m]}\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(answer_dataframe)\n",
      "File \u001b[1;32mc:\\users\\user\\ml-spring-2023\\lib\\site-packages\\pandas\\core\\frame.py:2906\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 2906\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   2908\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\users\\user\\ml-spring-2023\\lib\\site-packages\\pandas\\core\\indexes\\base.py:2900\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2898\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   2899\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 2900\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   2902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tolerance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2903\u001b[0m     tolerance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_tolerance(tolerance, np\u001b[38;5;241m.\u001b[39masarray(key))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'index'"
     ]
    }
   ],
   "source": [
    "def project_1_scoring(data):\n",
    "    # replace Na/Null values\n",
    "    values_to_fill = {}\n",
    "    for col in data.drop(columns=['MIS_Status']).columns:\n",
    "        if data[col].dtype == 'object':\n",
    "            values_to_fill[col] = \"Missing\"\n",
    "        else:\n",
    "            values_to_fill[col] = 0\n",
    "\n",
    "    data.fillna(value=values_to_fill,inplace=True)\n",
    "    \n",
    "    #Converting the strings styled as '$XXXX.XX' to float values.\n",
    "    data['DisbursementGross'] = data['DisbursementGross'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "    data['BalanceGross'] = data['BalanceGross'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "    data['GrAppv'] = data['GrAppv'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "    data['SBA_Appv'] = data['SBA_Appv'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "    \n",
    "    data['MIS_Status'] = data['MIS_Status'].replace({'CHGOFF': 1, 'P I F': 0}).astype(float)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    #Adding Engineered Features\n",
    "    \n",
    "    #1. Creating a feature that is indicating whether the borrower is located in an urban or rural area\n",
    "    data['Is_urban'] = (data['UrbanRural'] == 1).astype(int)\n",
    "    \n",
    "    #2. Creating a feature that is indicating the ratio of retained jobs to created jobs\n",
    "    data['Retained_Created_Job_Ratio'] = data['RetainedJob'] /(data['CreateJob'] + 1)\n",
    "    \n",
    "    #3. Creating a feature that is indicating whether the borrower has a low documentation loan\n",
    "    data['Is_low_doc'] = (data['LowDoc'] == 'Y').astype(int)\n",
    "    \n",
    "    #4. Creating a feature that is indicating whether the borrower has a revolving line of credit\n",
    "    data['Is_rev_line'] = (data['RevLineCr'] == 'Y').astype(int)\n",
    "    \n",
    "    #5. Creating a feature that is indicating the ratio of the loan amount to the gross disbursement\n",
    "    data['Loan_Gross_Ratio'] = data['GrAppv'] / data['DisbursementGross']\n",
    "    \n",
    "    #6. Creating a feature that is indicating the ratio of the SBA loan amount to the gross disbursement\n",
    "    data['SBA_Loan_Gross_Ratio'] = data['SBA_Appv'] / data['DisbursementGross']\n",
    "    \n",
    "    #7. Creating a binary variable feature that is indicating whether the loan was for a franchise\n",
    "    data['Is_franchise'] = (data['FranchiseCode'] != 0).astype(int)    \n",
    "    \n",
    "    #8. Creating a feature that is log transformation of Disbursement Gross\n",
    "    data['LogDisbursementGross'] = np.log(data['DisbursementGross'] + 1)\n",
    "    \n",
    "    #9. Creating a feature that is Log Transformation of SBA Approval Amount\n",
    "    data['LogSBAApprovalAmount'] = np.log(data['SBA_Appv'] + 1)\n",
    "\n",
    "    #10. Creating a feature that is indicating the bank originated Loan\n",
    "    data['BankOriginatedLoan'] = np.where(data['Bank'] == data['BankState'], 1, 0)\n",
    "\n",
    "    #11. Creating a feature that is indicating loan amount to income ratio or disbursement amount per employees\n",
    "    data['LoanToIncomeRatio'] = data['DisbursementGross'] / (data['NoEmp'] + 1)\n",
    "\n",
    "    #12. Creating a feature that is indicating loan amount to number of business owners Ratio\n",
    "    data['LoanToOwnerRatio'] = data['DisbursementGross'] / (data['NewExist'] + 1)\n",
    "    \n",
    "    \n",
    "    #Categorical encoders dictionary\n",
    "    cat_encoders = {}\n",
    "    #New categorical (encoded) columns\n",
    "    cat_enc_columns = []\n",
    "\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype == 'object':\n",
    "            if data[col].nunique() < 10:\n",
    "                print(\"One-hot encoding of \", col)\n",
    "                enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "                enc.fit(data[[col]])\n",
    "                result = enc.transform(data[[col]])\n",
    "                ohe_columns = [col+\"_\"+str(x) for x in enc.categories_[0]]\n",
    "                cat_enc_columns = cat_enc_columns + ohe_columns\n",
    "                result_df = pd.DataFrame(result, columns=ohe_columns, index=data.index)\n",
    "                data = pd.concat([data, result_df.reindex(data.index)], axis=1, join='inner')\n",
    "                cat_encoders[col] = [deepcopy(enc), \"ohe\"]\n",
    "            else:\n",
    "                print(\"Target encoding of \", col)\n",
    "                enc = TargetEncoder()\n",
    "                enc.fit(data[col], y=data['MIS_Status'], handle_unknown='value')\n",
    "                pickle.dump(enc, open(col+'_trg_'+'pre_processing.p', \"wb\"))\n",
    "                new_col_name = col+\"_trg\"\n",
    "                data[new_col_name] = enc.transform(data[[col]])\n",
    "                cat_encoders[col] = [deepcopy(enc), \"trg\"]\n",
    "                cat_enc_columns.append(new_col_name)\n",
    "                \n",
    "    \n",
    "    col20 = data.pop(data.columns[20])\n",
    "    data.insert(1, col20.name, col20)\n",
    "    \n",
    "    \n",
    "    num_scalers = {}\n",
    "\n",
    "    '''Scaling only original and feature engineered columns'''\n",
    "    for col in data.columns[2:33]:\n",
    "        if data[col].dtype != 'object':\n",
    "            print(\"Standard scale of \", col)\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(data[[col]])\n",
    "            pickle.dump(scaler, open(col+'_sc_'+'pre_processing.p', \"wb\"))\n",
    "            data[col+\"_sc\"] = scaler.transform(data[[col]])\n",
    "        \n",
    "            num_scalers[col] = [deepcopy(scaler),\"Standard\"]\n",
    "            \n",
    "            \n",
    "    #Splitting the dataset into train (60%), validation (20%), and test (20%) sets\n",
    "    train_val, test = train_test_split(data, test_size=0.2, random_state=182)\n",
    "    train, val = train_test_split(train_val, test_size=0.25, random_state=182)\n",
    "    \n",
    "    X_train = train.drop(columns='MIS_Status')\n",
    "    y_train = train['MIS_Status']\n",
    "    X_valid = val.drop(columns='MIS_Status')\n",
    "    y_valid = val['MIS_Status']\n",
    "    X_test = test.drop(columns='MIS_Status')\n",
    "    y_test = test['MIS_Status']\n",
    "    X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape\n",
    "    \n",
    "    X_trn = X_train.iloc[:,33:]\n",
    "    X_vld = X_valid.iloc[:,33:]\n",
    "    X_tst = X_test.iloc[:,33:]\n",
    "    \n",
    "    \n",
    "    filepath = os.path.join(dirname, '../artifacts/artifacts_dict_file.pkl')\n",
    "    # working with the artifact file to import the model and threshold\n",
    "    artifacts_dict_file = open(filepath, \"rb\")\n",
    "    artifacts_dict = pickle.load(file=artifacts_dict_file)\n",
    "    artifacts_dict_file.close()\n",
    "    logreg = artifacts_dict[\"model\"]\n",
    "    threshold = artifacts_dict[\"threshold\"]\n",
    "    \n",
    "    y_pred_prob = logreg.predict_proba(X_trn)\n",
    "    y_pred = (y_pred_prob[:,0] < threshold).astype(np.int16)\n",
    "    answer_dataframe = {\"index\":X_trn[\"index\"],\n",
    "         \"label\":y_pred,\n",
    "         \"probability_0\":y_pred_prob[:,0],\n",
    "         \"probability_1\":y_pred_prob[:,1]}\n",
    "    \n",
    "\n",
    "    return pd.DataFrame(answer_dataframe)\n",
    "\n",
    "#reading the data and keeping the new test data file in the same folder\n",
    "datafilepath = os.path.join(dirname, './notebooks/SBA_loans_project_1.zip')\n",
    "data = pd.read_csv(datafilepath)\n",
    "# calling the function\n",
    "answer=project_1_scoring(data)\n",
    "# printing the answer\n",
    "print(answer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd486b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecdcccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
